{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487417e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Hello World!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'Hello World!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b703b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0+cu126\n",
      "True\n",
      "1\n",
      "0\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df9d54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0718721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, size=64):\n",
    "        self.files = glob(os.path.join(root, \"**/*,jpg\"), recursive=True)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(size),\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
    "        return self.transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a459767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, z_dim=512, w_dim=512):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(8):\n",
    "            layers.append(nn.Linear(w_dim, w_dim))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "        self.mapping = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        z = z / z.norm(dim=1, keepdim=True)\n",
    "        return self.mapping(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80e79ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, w_dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_ch, in_ch, 3, 3))\n",
    "        self.style = nn.Linear(w_dim, in_ch)\n",
    "        self.noise_strength = nn.Parameter(torch.zeros(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_ch))\n",
    "    \n",
    "    def forward(self, x, w, noise):\n",
    "        b, c, h, w_ = x.shape\n",
    "        style = self.style(w).view(b, 1, c, 1, 1)\n",
    "        weight = self.weight.unsqueeze(0) * style\n",
    "        weight = weight.view(-1, c, 3, 3)\n",
    "\n",
    "        x = x.view(1, -1, h, w_)\n",
    "        x= F.conv2d(x, weight, padding=1, groups=b)\n",
    "        x = x.view(b, -1, h, w_)\n",
    "\n",
    "        x = x + self.noise_strength * noise\n",
    "        return x + self.bias.view(1, -1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bcdb7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=512, w_dim=512):\n",
    "        super().__init__()\n",
    "        self.mapping = MappingNetwork(z_dim, w_dim)\n",
    "\n",
    "        self.const = nn.Parameter(torch.randn(1, 512, 4, 4))\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            StyleConv(512, 512, w_dim),\n",
    "            StyleConv(512, 256, w_dim),\n",
    "            StyleConv(256, 128, w_dim),\n",
    "            StyleConv(128, 64, w_dim),\n",
    "          ])\n",
    "        \n",
    "        self.to_rgb = nn.Conv2d(64, 3, 1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        w = self.mapping(z)\n",
    "        x = self.const.repeat(z.size(0), 1, 1, 1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            noise = torch.randn_like(x[:, :1])\n",
    "            x = F.interpolate(x, scale_factor=2, mode='bilinear')\n",
    "            x = layer(x, w, noise)\n",
    "            x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        return torch.tanh(self.to_rgb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52080e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        def block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 4, 2, 1),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            block(3, 64),\n",
    "            block(64, 128),\n",
    "            block(128, 256),\n",
    "            block(256, 512),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512*4*4, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43f61629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_Penalty(D, real, fake):\n",
    "    alpha = torch.rand(real.size(0), 1, 1, 1).to(device)\n",
    "    interp = (alpha * real + (1 - alpha) * fake).requires_grad_(True)\n",
    "    out = D(interp)\n",
    "\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=out,\n",
    "        inputs=interp,\n",
    "        grad_outputs=torch.ones_like(out),\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    return ((grads.norm(2, dim=1) - 1) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c25856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    path = r\"C:\\Users\\TEJAS KUMAR V URS\\Desktop\\MSc Data Science\\Projects\\Animal Gen AI\\Dataset\"\n",
    "    dataset = ImageDataset(path)\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "\n",
    "    G = Generator().to(device)\n",
    "    D = Discriminator().to(device)\n",
    "\n",
    "    g_opt = torch.optim.Adam(G.parameters(), lr=1e-4, betas=(0.0, 0.99))\n",
    "    d_opt = torch.optim.Adam(D.parameters(), lr=1e-4, betas=(0.0, 0.99))\n",
    "\n",
    "    for epoch in range(50):\n",
    "        for i, real in enumerate(loader):\n",
    "            real = real.to(device)\n",
    "            z = torch.randn(real.size(0), 512).to(device)\n",
    "            fake = G(z)\n",
    "\n",
    "            d_loss = D(fake).mean() - D(real).mean()\n",
    "            gp = Gradient_Penalty(D, real, fake)\n",
    "            d_total = d_loss + 10 * gp\n",
    "\n",
    "            d_opt.zero_grad()\n",
    "            d_total.backward()\n",
    "            d_opt.step()\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                g_loss = -D(G(z)).mean()\n",
    "                g_opt.zero_grad()\n",
    "                g_loss.backward()\n",
    "                g_opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            samples = G(torch.randn(16, 512).to(device))\n",
    "            save_image(samples, f\"samples/epoch_{epoch}.png\", normalize=True)\n",
    "\n",
    "        print(f\"Epoch {epoch} | D: {d_total.item():.3f} | G: {g_loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d05d55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"--main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43285d4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\TEJAS KUMAR V URS\\\\Desktop\\\\MSc Data Science\\\\Projects\\\\Animal Gen AI\\\\Dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2649754297.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"C:\\Users\\TEJAS KUMAR V URS\\Desktop\\MSc Data Science\\Projects\\Animal Gen AI\\Dataset\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of images found:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\TEJAS KUMAR V URS\\\\Desktop\\\\MSc Data Science\\\\Projects\\\\Animal Gen AI\\\\Dataset'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186262c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
