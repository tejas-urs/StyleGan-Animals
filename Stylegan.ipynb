{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b487417e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Hello World!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Hello World!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b703b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0+cu126\n",
      "True\n",
      "1\n",
      "0\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df9d54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from glob import glob\n",
    "import os\n",
    "import kagglehub\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2187751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.1).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/tejaskumarvurs/gen-ai-animal-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.91G/8.91G [01:39<00:00, 96.4MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset is ready at: /content/animal_data\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"KAGGLE_USERNAME\"] = \"tejaskumarvurs\"\n",
    "os.environ[\"KAGGLE_KEY\"] = \"80fbc5d540819df3b4666ae5df969af9\"\n",
    "\n",
    "path = kagglehub.dataset_download(\"tejaskumarvurs/gen-ai-animal-dataset\")\n",
    "\n",
    "dest = \"/content/animal_data\"\n",
    "if not os.path.exists(dest):\n",
    "    shutil.copytree(path, dest)\n",
    "\n",
    "print(f\"‚úÖ Dataset is ready at: {dest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12c8a211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/kagglehub/datasets/tejaskumarvurs/gen-ai-animal-dataset\n"
     ]
    }
   ],
   "source": [
    "!find / -name \"gen-ai-animal-dataset\" -type d 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06f51fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/content/dataset/1': File exists\n",
      "1\n",
      "Bear\n",
      "Brown bear\n",
      "Bull\n",
      "Butterfly\n",
      "01d5030b1bb698d6.jpg\n",
      "0215b972cb19e575.jpg\n",
      "02db256a75c9419a.jpg\n",
      "035205673c0ae617.jpg\n",
      "049a26d67bd3192d.jpg\n"
     ]
    }
   ],
   "source": [
    "# Force a link from the hidden system path to your visible content folder\n",
    "!ln -s /root/.cache/kagglehub/datasets/tejaskumarvurs/gen-ai-animal-dataset/versions/1 /content/dataset\n",
    "\n",
    "# Now check if Colab can 'see' into that shortcut\n",
    "!ls /content/dataset | head -n 5\n",
    "\n",
    "!ls /content/dataset/Camel | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a694740e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files accessible for T4 training: 29071\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"/content/dataset\"\n",
    "\n",
    "def count_files(directory):\n",
    "    return sum([len(files) for r, d, files in os.walk(directory)])\n",
    "\n",
    "print(f\"Total files accessible for T4 training: {count_files(DATA_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c640ca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Path exists!\n",
      "Items inside '/content/dataset': ['Camel', 'Bear', 'Lynx', 'Woodpecker', 'Zebra']\n",
      "üìÅ Found subfolders (Classes). Use datasets.ImageFolder(path)\n"
     ]
    }
   ],
   "source": [
    "path = \"/content/dataset\"\n",
    "\n",
    "if os.path.exists(path):\n",
    "    contents = os.listdir(path)\n",
    "    print(f\"‚úÖ Path exists!\")\n",
    "    print(f\"Items inside '{path}': {contents[:5]}\") # Shows first 5 items\n",
    "    \n",
    "    # Check if the first item is a file or a folder\n",
    "    if len(contents) > 0:\n",
    "        first_item = os.path.join(path, contents[0])\n",
    "        if os.path.isdir(first_item):\n",
    "            print(\"üìÅ Found subfolders (Classes). Use datasets.ImageFolder(path)\")\n",
    "        else:\n",
    "            print(\"üñºÔ∏è Found direct files. Use a custom Dataset class.\")\n",
    "else:\n",
    "    print(f\"‚ùå Path NOT found: {path}\")\n",
    "    print(\"Checking /content/ to see what is actually there:\")\n",
    "    print(os.listdir(\"/content/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "548a3dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content/dataset/Camel/1431422309330f42.jpg', '/content/dataset/Camel/0215b972cb19e575.jpg', '/content/dataset/Camel/08106ac0bce4f1c0.jpg', '/content/dataset/Camel/408cd6b071fd0546.jpg', '/content/dataset/Camel/962a74b1418c8f14.jpg', '/content/dataset/Camel/e9c80cc9d12c6992.jpg', '/content/dataset/Camel/76f8fac151aa253f.jpg', '/content/dataset/Camel/e18dedb8deb8a34b.jpg', '/content/dataset/Camel/4bc1027f1ccb353d.jpg', '/content/dataset/Camel/b49844caf0ecc14c.jpg', '/content/dataset/Camel/17694cf57865e6b1.jpg', '/content/dataset/Camel/02db256a75c9419a.jpg', '/content/dataset/Camel/f8b69bc9cb1d75ef.jpg', '/content/dataset/Camel/17648cf45f4ab8cd.jpg', '/content/dataset/Camel/d741344d81466170.jpg', '/content/dataset/Camel/0f7062aec3674569.jpg', '/content/dataset/Camel/1c68ec73964a0332.jpg', '/content/dataset/Camel/392f07b11b16ab47.jpg', '/content/dataset/Camel/3a3dabb794c13492.jpg', '/content/dataset/Camel/b591487db3b1bb2a.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 20 file paths to see if they look correct\n",
    "test_ds = ImageDataset(\"/content/dataset\")\n",
    "print(test_ds.files[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0718721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, size=64):\n",
    "        # Fixed the typo: \"**/*.jpg\" instead of \"**/*,jpg\"\n",
    "        self.files = glob(os.path.join(root, \"**/*.jpg\"), recursive=True)\n",
    "        \n",
    "        if len(self.files) == 0:\n",
    "            print(f\"‚ö†Ô∏è Warning: No .jpg files found in {root}. Check path or extensions.\")\n",
    "            \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(size),\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3) # Scales to [-1, 1]\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
    "        return self.transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a459767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, z_dim=512, w_dim=512):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(8):\n",
    "            layers.append(nn.Linear(w_dim, w_dim))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "        self.mapping = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        z = z / z.norm(dim=1, keepdim=True)\n",
    "        return self.mapping(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80e79ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, w_dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_ch, in_ch, 3, 3))\n",
    "        self.style = nn.Linear(w_dim, in_ch)\n",
    "        self.noise_strength = nn.Parameter(torch.zeros(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_ch))\n",
    "    \n",
    "    def forward(self, x, w, noise):\n",
    "        b, c, h, w_ = x.shape\n",
    "        style = self.style(w).view(b, 1, c, 1, 1)\n",
    "        weight = self.weight.unsqueeze(0) * style\n",
    "        weight = weight.view(-1, c, 3, 3)\n",
    "\n",
    "        x = x.view(1, -1, h, w_)\n",
    "        x= F.conv2d(x, weight, padding=1, groups=b)\n",
    "        x = x.view(b, -1, h, w_)\n",
    "\n",
    "        x = x + self.noise_strength * noise\n",
    "        return x + self.bias.view(1, -1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bcdb7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=512, w_dim=512):\n",
    "        super().__init__()\n",
    "        self.mapping = MappingNetwork(z_dim, w_dim)\n",
    "\n",
    "        self.const = nn.Parameter(torch.randn(1, 512, 4, 4))\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            StyleConv(512, 512, w_dim),\n",
    "            StyleConv(512, 256, w_dim),\n",
    "            StyleConv(256, 128, w_dim),\n",
    "            StyleConv(128, 64, w_dim),\n",
    "          ])\n",
    "        \n",
    "        self.to_rgb = nn.Conv2d(64, 3, 1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        w = self.mapping(z)\n",
    "        x = self.const.repeat(z.size(0), 1, 1, 1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            noise = torch.randn_like(x[:, :1])\n",
    "            x = F.interpolate(x, scale_factor=2, mode='bilinear')\n",
    "            x = layer(x, w, noise)\n",
    "            x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        return torch.tanh(self.to_rgb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52080e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        def block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 4, 2, 1),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            block(3, 64),\n",
    "            block(64, 128),\n",
    "            block(128, 256),\n",
    "            block(256, 512),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512*4*4, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43f61629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_Penalty(D, real, fake):\n",
    "    alpha = torch.rand(real.size(0), 1, 1, 1).to(device)\n",
    "    interp = (alpha * real + (1 - alpha) * fake).requires_grad_(True)\n",
    "    out = D(interp)\n",
    "\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=out,\n",
    "        inputs=interp,\n",
    "        grad_outputs=torch.ones_like(out),\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    return ((grads.norm(2, dim=1) - 1) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c25856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    path = \"/content/dataset\"  # Update this path as needed\n",
    "    dataset = ImageDataset(path, size=64)\n",
    "    print(f\"‚úÖ Dataset loaded with {len(dataset)} images.\")\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "    G = Generator().to(device)\n",
    "    D = Discriminator().to(device)\n",
    "\n",
    "    g_opt = torch.optim.Adam(G.parameters(), lr=1e-4, betas=(0.0, 0.99))\n",
    "    d_opt = torch.optim.Adam(D.parameters(), lr=1e-4, betas=(0.0, 0.99))\n",
    "\n",
    "    for epoch in range(50):\n",
    "        for i, real in enumerate(loader):\n",
    "            real = real.to(device)\n",
    "            z = torch.randn(real.size(0), 512).to(device)\n",
    "            fake = G(z)\n",
    "\n",
    "            d_loss = D(fake).mean() - D(real).mean()\n",
    "            gp = Gradient_Penalty(D, real, fake)\n",
    "            d_total = d_loss + 10 * gp\n",
    "\n",
    "            d_opt.zero_grad()\n",
    "            d_total.backward()\n",
    "            d_opt.step()\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                g_loss = -D(G(z)).mean()\n",
    "                g_opt.zero_grad()\n",
    "                g_loss.backward()\n",
    "                g_opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            samples = G(torch.randn(16, 512).to(device))\n",
    "            save_image(samples, f\"samples/epoch_{epoch}.png\", normalize=True)\n",
    "\n",
    "        print(f\"Epoch {epoch} | D: {d_total.item():.3f} | G: {g_loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d05d55b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded with 1162840 images.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (4) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1990237592.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-1350851622.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2604015425.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1995271918.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, w, noise)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_strength\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (4) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e767acd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
