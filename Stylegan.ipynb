{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487417e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Hello World!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Hello World!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac521b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use this when the folder is not created in the drive and also you need to mount\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "drive_path = \"/content/drive/MyDrive/GAN_Project\"\n",
    "os.makedirs(drive_path, exist_ok=True)\n",
    "\n",
    "# testing file\n",
    "test_file = os.path.join(drive_path, \"connection_test.txt\")\n",
    "with open(test_file, \"w\") as f:\n",
    "    f.write(\"Drive is connected!\")\n",
    "\n",
    "if os.path.exists(test_file):\n",
    "    print(\"‚úÖ DRIVE VERIFIED: Connection is live. You can start training.\")\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå DRIVE FAILURE: Files are not writing to Drive. Check permissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535bbed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount and veriy only when the folder is already created in the drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "drive_save_path = \"/content/drive/MyDrive/GAN_Project\"\n",
    "checkpoint_path = os.path.join(drive_save_path, \"checkpoint_epoch_9.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b703b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, z_dim=512, w_dim=512):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(8):\n",
    "            layers.append(nn.Linear(w_dim, w_dim))\n",
    "            layers.append(nn.LeakyReLU(0.2)) # Standard StyleGAN leakiness\n",
    "        self.mapping = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # Normalize the input latent vector\n",
    "        z = z / (z.norm(dim=1, keepdim=True) + 1e-8)\n",
    "        return self.mapping(z)\n",
    "\n",
    "class StyleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, w_dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_ch, in_ch, 3, 3))\n",
    "        self.style = nn.Linear(w_dim, in_ch)\n",
    "        self.noise_strength = nn.Parameter(torch.zeros(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_ch))\n",
    "    \n",
    "    def forward(self, x, w, noise):\n",
    "        b, c, h, w_ = x.shape\n",
    "        # Weight demodulation/modulation logic\n",
    "        style = self.style(w).view(b, 1, c, 1, 1)\n",
    "        weight = self.weight.unsqueeze(0) * style\n",
    "        weight = weight.view(-1, c, 3, 3)\n",
    "\n",
    "        x = x.view(1, -1, h, w_)\n",
    "        x = F.conv2d(x, weight, padding=1, groups=b)\n",
    "        x = x.view(b, -1, h, w_)\n",
    "\n",
    "        x = x + self.noise_strength * noise\n",
    "        return x + self.bias.view(1, -1, 1, 1)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=512, w_dim=512):\n",
    "        super().__init__()\n",
    "        self.mapping = MappingNetwork(z_dim, w_dim)\n",
    "        self.const = nn.Parameter(torch.randn(1, 512, 4, 4))\n",
    "        self.layers = nn.ModuleList([\n",
    "            StyleConv(512, 512, w_dim),\n",
    "            StyleConv(512, 256, w_dim),\n",
    "            StyleConv(256, 128, w_dim),\n",
    "            StyleConv(128, 64, w_dim),\n",
    "        ])\n",
    "        self.to_rgb = nn.Conv2d(64, 3, 1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        w = self.mapping(z)\n",
    "        x = self.const.repeat(z.size(0), 1, 1, 1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device)\n",
    "            x = layer(x, w, noise)\n",
    "            x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        return torch.tanh(self.to_rgb(x))\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        def block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 4, 2, 1),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        self.net = nn.Sequential(\n",
    "            block(3, 64),\n",
    "            block(64, 128),\n",
    "            block(128, 256),\n",
    "            block(256, 512),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512*4*4, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e420d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import torch\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, size=64):\n",
    "        # Fixed the glob import usage\n",
    "        self.files = glob(os.path.join(root, \"**/*.jpg\"), recursive=True)\n",
    "        \n",
    "        if len(self.files) == 0:\n",
    "            print(f\"‚ö†Ô∏è Warning: No .jpg files found in {root}. Check path or extensions.\")\n",
    "            \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(size),\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3) # Scales to [-1, 1]\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
    "        return self.transform(img)\n",
    "\n",
    "def Gradient_Penalty(D, real, fake):\n",
    "    # CRITICAL: We need to know which device to use\n",
    "    device = real.device \n",
    "    \n",
    "    alpha = torch.rand(real.size(0), 1, 1, 1).to(device)\n",
    "    interp = (alpha * real + (1 - alpha) * fake).requires_grad_(True)\n",
    "    out = D(interp)\n",
    "\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=out,\n",
    "        inputs=interp,\n",
    "        grad_outputs=torch.ones_like(out),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    # Calculate the norm correctly across the image dimensions\n",
    "    grads = grads.view(grads.size(0), -1)\n",
    "    gp = ((grads.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2187751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.3).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/tejaskumarvurs/gen-ai-animal-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.91G/8.91G [01:26<00:00, 110MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset is ready at: /content/animal_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from kagglehub import kagglehub\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"tejaskumarvurs\"\n",
    "os.environ[\"KAGGLE_KEY\"] = \"80fbc5d540819df3b4666ae5df969af9\"\n",
    "\n",
    "path = kagglehub.dataset_download(\"tejaskumarvurs/gen-ai-animal-dataset\")\n",
    "\n",
    "dest = \"/content/animal_data\"\n",
    "if not os.path.exists(dest):\n",
    "    shutil.copytree(path, dest)\n",
    "\n",
    "print(f\"‚úÖ Dataset is ready at: {dest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c8a211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/kagglehub/datasets/tejaskumarvurs/gen-ai-animal-dataset\n"
     ]
    }
   ],
   "source": [
    "!find / -name \"gen-ai-animal-dataset\" -type d 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f51fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bear\n",
      "Brown bear\n",
      "Bull\n",
      "Butterfly\n",
      "Camel\n",
      "01d5030b1bb698d6.jpg\n",
      "0215b972cb19e575.jpg\n",
      "02db256a75c9419a.jpg\n",
      "035205673c0ae617.jpg\n",
      "049a26d67bd3192d.jpg\n"
     ]
    }
   ],
   "source": [
    "# Force a link from the hidden system path to your visible content folder\n",
    "!ln -s /root/.cache/kagglehub/datasets/tejaskumarvurs/gen-ai-animal-dataset/versions/1 /content/dataset\n",
    "\n",
    "# Now check if Colab can 'see' into that shortcut\n",
    "!ls /content/dataset | head -n 5\n",
    "\n",
    "!ls /content/dataset/Camel | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a694740e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files accessible for T4 training: 29071\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"/content/dataset\"\n",
    "\n",
    "def count_files(directory):\n",
    "    return sum([len(files) for r, d, files in os.walk(directory)])\n",
    "\n",
    "print(f\"Total files accessible for T4 training: {count_files(DATA_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c640ca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Path exists!\n",
      "Items inside '/content/dataset': ['Crab', 'Sea turtle', 'Parrot', 'Elephant', 'Koala']\n",
      "üìÅ Found subfolders (Classes). Use datasets.ImageFolder(path)\n"
     ]
    }
   ],
   "source": [
    "path = \"/content/dataset\"\n",
    "\n",
    "if os.path.exists(path):\n",
    "    contents = os.listdir(path)\n",
    "    print(f\"‚úÖ Path exists!\")\n",
    "    print(f\"Items inside '{path}': {contents[:5]}\") # Shows first 5 items\n",
    "    \n",
    "    # Check if the first item is a file or a folder\n",
    "    if len(contents) > 0:\n",
    "        first_item = os.path.join(path, contents[0])\n",
    "        if os.path.isdir(first_item):\n",
    "            print(\"üìÅ Found subfolders (Classes). Use datasets.ImageFolder(path)\")\n",
    "        else:\n",
    "            print(\"üñºÔ∏è Found direct files. Use a custom Dataset class.\")\n",
    "else:\n",
    "    print(f\"‚ùå Path NOT found: {path}\")\n",
    "    print(\"Checking /content/ to see what is actually there:\")\n",
    "    print(os.listdir(\"/content/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d375328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils\n",
    "import model\n",
    "\n",
    "# This forces the notebook to read the new versions of your files, these are required when model or utils have been changed.\n",
    "importlib.reload(utils)\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import Generator, Discriminator\n",
    "from utils import ImageDataset, Gradient_Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c25856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from glob import glob\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    path = \"/content/dataset\" \n",
    "    \n",
    "    # Ensure local and Drive folders exist\n",
    "    os.makedirs(\"samples\", exist_ok=True)\n",
    "    drive_save_path = \"/content/drive/MyDrive/GAN_Project\"\n",
    "    os.makedirs(drive_save_path, exist_ok=True)\n",
    "\n",
    "    dataset = ImageDataset(path, size=64)\n",
    "    print(f\"‚úÖ Dataset loaded with {len(dataset)} images.\")\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "    G = Generator().to(device)\n",
    "    D = Discriminator().to(device)\n",
    "\n",
    "    g_opt = torch.optim.Adam(G.parameters(), lr=1e-4, betas=(0.0, 0.99))\n",
    "    d_opt = torch.optim.Adam(D.parameters(), lr=1e-4, betas=(0.0, 0.99))\n",
    "\n",
    "    # Initialize g_loss so the print statement doesn't crash\n",
    "    g_loss = torch.tensor(0.0)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        for i, real in enumerate(loader):\n",
    "            real = real.to(device)\n",
    "            z = torch.randn(real.size(0), 512).to(device)\n",
    "            fake = G(z)\n",
    "\n",
    "            # Discriminator Update\n",
    "            d_loss = D(fake).mean() - D(real).mean()\n",
    "            gp = Gradient_Penalty(D, real, fake)\n",
    "            d_total = d_loss + 10 * gp\n",
    "\n",
    "            d_opt.zero_grad()\n",
    "            d_total.backward()\n",
    "            d_opt.step()\n",
    "\n",
    "            # Generator Update (Every 5 steps)\n",
    "            if i % 5 == 0:\n",
    "                # We need to re-generate or detach if we used fake above\n",
    "                g_loss = -D(G(z)).mean()\n",
    "                g_opt.zero_grad()\n",
    "                g_loss.backward()\n",
    "                g_opt.step()\n",
    "\n",
    "        # --- PERIODIC SAVE (Safety Net) ---\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            ckpt_path = os.path.join(drive_save_path, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'G_state': G.state_dict(),\n",
    "                'D_state': D.state_dict(),\n",
    "                'g_opt': g_opt.state_dict(),\n",
    "                'd_opt': d_opt.state_dict()\n",
    "            }, ckpt_path)\n",
    "            print(f\"üíæ Checkpoint saved: {ckpt_path}\")\n",
    "\n",
    "        # Save Sample Images\n",
    "        with torch.no_grad():\n",
    "            samples = G(torch.randn(16, 512).to(device))\n",
    "            save_image(samples, f\"samples/epoch_{epoch}.png\", normalize=True)\n",
    "\n",
    "        print(f\"Epoch {epoch} | D: {d_total.item():.3f} | G: {g_loss.item():.3f}\")\n",
    "\n",
    "    # Final Save\n",
    "    torch.save(G.state_dict(), os.path.join(drive_save_path, \"generator_final.pth\"))\n",
    "    print(\"üèÅ Training complete. Final model saved to Drive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5064ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded with 29071 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | D: -126.259 | G: -58.254\n",
      "Epoch 1 | D: -44.432 | G: -135.634\n",
      "Epoch 2 | D: -72.408 | G: -189.737\n",
      "Epoch 3 | D: -88.329 | G: -176.951\n",
      "Epoch 4 | D: -1.748 | G: 79.333\n",
      "Epoch 5 | D: -6.057 | G: -17.648\n",
      "Epoch 6 | D: -12.065 | G: -42.308\n",
      "Epoch 7 | D: -15.027 | G: -48.577\n",
      "Epoch 8 | D: -8.847 | G: -107.692\n",
      "üíæ Checkpoint saved: /content/drive/MyDrive/GAN_Project/checkpoint_epoch_9.pth\n",
      "Epoch 9 | D: -10.874 | G: -73.063\n",
      "Epoch 10 | D: -13.779 | G: -42.733\n",
      "Epoch 11 | D: -9.729 | G: -22.208\n",
      "Epoch 12 | D: -3.548 | G: -11.995\n",
      "Epoch 13 | D: -7.448 | G: -24.200\n",
      "Epoch 14 | D: -6.425 | G: -6.672\n",
      "Epoch 15 | D: -4.407 | G: -39.681\n",
      "Epoch 16 | D: -6.710 | G: -16.082\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
